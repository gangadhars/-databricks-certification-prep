# Databricks Certified Data Engineer Associate

Use this version of the exam guide if you are taking your exam on or AFTER July 25th. [[PDF: databricks-certified-data-engineer-associate-exam-guide-25.pdf](https://www.databricks.com/sites/default/files/2025-07/databricks-certified-data-engineer-associate-exam-guide-25.pdf)]

## About the Exam

* Number of scored items: 45 multiple-choice questions
* Time limit: 90 minutes
* Registration fee: USD 200, plus applicable taxes as required per local law
* Delivery method: Online Proctored
* Test aides: none allowed.
* Prerequisite: None required; course attendance and six months of hands-on experience in Databricks is highly recommended
* Validity: 2 years
* Recertification: Recertification is required every two years to maintain your certified status. To recertify, you must take the full exam that is currently live. Please review the “Getting Ready for the Exam” section on the exam webpage to prepare for taking the exam again.
* Unscored Content: Exams may include unscored items to gather statistical information for future use. These items are not identified on the form and do not impact your score. Additional time is factored into account for this content.

## Exam outline

### Section 1: Databricks Intelligence Platform

* Enable features that simplify data layout decisions and optimize query performance.
* Explain the value of the Data Intelligence Platform.
* Identify the applicable compute to use for a specific use case.

### Section 2: Development and Ingestion

* Use Databricks Connect in a data engineering workflow
* Determine the capabilities of Notebooks functionality
* Classify valid Auto Loader sources and use cases
* Demonstrate knowledge of Auto Loader syntax
* Use Databricks' built-in debugging tools to troubleshoot a given issue

### Section 3: Data Processing & Transformations

* Describe the three layers of the Medallion Architecture and explain the purpose of each layer in a data processing pipeline.
* Classify the type of the cluster and configuration for optimal performance based on the
scenario on which cluster is used.
* Emphasize the advantages of DLT (for ETL process in Databricks).
* Implement data pipelines using DLT..
* Identify DDL (Data Definition Language)/DML features.
* Compute complex aggregations and Metrics with PySpark Dataframes.

### Section 4: Productionizing Data Pipelines

* Identify the difference between DAB and traditional deployment methods.
* Identify the structure of Asset Bundles.
* Deploy a workflow, repair, and rerun a task in case of failure.
* Use serverless for a hands-off, auto-optimized compute managed by Databricks.
* Analyzing the Spark UI to optimize the query.

## Section 5: Data Governance & Quality

* Explain the difference between managed and external tables.
* Identify the grant of permissions to users and groups within UC.
* Identify key roles in UC.
* Identify how audit logs are stored.
* Use lineage features in Unity Catalog.
* Use the Delta Sharing feature available with Unity Catalog to share data.
* Identify the advantages and limitations of Delta sharing.
* Identify types of delta sharing- Databricks vs external system.
* Analyze the cost considerations of data sharing across clouds
* Identify Use cases of Lakehouse Federation when connected to external sources.

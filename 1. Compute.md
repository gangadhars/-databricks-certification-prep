# Databricks Compute: Comprehensive Notes for Certification

## 1. Foundational Concepts

### 1.1. The Databricks Lakehouse Platform

The Databricks Lakehouse Platform unifies data warehousing and AI use cases on a single platform. It combines the reliability, governance, and performance of data warehouses with the openness, flexibility, and machine learning support of data lakes.

*   **Databricks Workspace:** The environment for accessing all your Databricks assets, organizing objects like notebooks and libraries into folders, and providing access to data and compute resources.
*   **Compute:** The cloud hardware resources (CPU, memory, storage) used to run workloads like ETL pipelines, queries, and ML model training.
*   **Delta Lake:** An open-source storage layer that brings ACID transactions and reliability to data lakes. It is the default format in Databricks and a core component for data engineering.
*   **Unity Catalog:** A unified governance solution for data and AI assets, providing centralized access control, auditing, and data discovery across workspaces.

### 1.2. Databricks Runtime (DBR)

The software that runs on clusters. It includes Apache Spark and other components that improve performance and security.

*   **Standard DBR:** For general-purpose data engineering and ETL workloads.
*   **DBR for Machine Learning (DBR ML):** Comes pre-packaged with popular ML libraries like TensorFlow, PyTorch, and scikit-learn, simplifying the environment setup for data scientists.
*   **GPU-Enabled DBRs:** Required for leveraging GPU hardware, these runtimes include the necessary NVIDIA drivers (CUDA, cuDNN).
*   **Long Term Support (LTS) Versions:** Recommended for production workloads to ensure stability and avoid compatibility issues from frequent updates. LTS versions are supported for three years.
*   **Beta Releases:** Provide early access to new features but are not recommended for production.

## 2. Cluster Fundamentals

A **cluster** is a set of compute resources (VMs) on which you run data engineering, data science, and data analytics workloads.

*   **Driver Node:** The "brain" that maintains the Spark context, interprets commands from a notebook or a job, and coordinates tasks across the worker nodes.
*   **Worker Nodes:** The "muscles" that execute the distributed data processing tasks assigned by the driver.
*   **Single-Node Cluster:** A cluster with a driver and zero workers. It's ideal for small data workloads or for using single-node libraries like pandas and scikit-learn.
*   **Multi-Node Cluster:** The standard for big data, used for parallel processing of large datasets.

## 3. Types of Compute

| Feature | All-Purpose Compute | Job Compute |
| :--- | :--- | :--- |
| **Primary Use Case** | Interactive analysis, collaborative development, ad-hoc queries. | Automated, scheduled production pipelines (ETL, batch). |
| **Lifecycle** | Persistent. Manually started, stopped, and restarted. | Ephemeral. Created and terminated automatically by the job scheduler. |
| **Restartable?** | Yes. | No. |
| **Cost (DBU Rate)** | Significantly higher. Billed for the entire time it is active. | Significantly lower. Optimized for cost-effective automated runs. |
| **User Interaction** | Multi-user, collaborative. | Isolated to a single job run. |

### 3.1. Serverless Compute

A fully managed option where Databricks handles all infrastructure, offering instant start-up and scaling. You only pay for what you use.

*   **Benefits:** Reduced management, instant compute, cost efficiency.
*   **Limitations:** Only supports Python and SQL. Does not support RDD APIs, JAR libraries, or compute-scoped init scripts. All data access must be through Unity Catalog.

### 3.2. SQL Warehouses

Compute resources specifically optimized for BI and SQL analytics workloads, powering the SQL Editor and connections from tools like Tableau and Power BI.

| Tier | Compute Location | Key Features | Best For |
| :--- | :--- | :--- | :--- |
| **Classic** | Customer's Account | Photon engine. | Entry-level interactive SQL. |
| **Pro** | Customer's Account | Photon, Predictive I/O. | Performance-sensitive BI workloads. |
| **Serverless** | Databricks's Account | Photon, Predictive I/O, Intelligent Workload Management (IWM). | High-concurrency, unpredictable, and cost-sensitive BI. |

### 3.3. Instance Pools

A set of idle, ready-to-use instances that reduce cluster start and auto-scaling times. Ideal for accelerating jobs with many short, sequential tasks.

### 3.4. Vector Search Compute (Preview)

Specialized compute for building and querying large-scale vector (embedding) indexes used in AI and search workloads.

## 4. Best Practices for Cost and Performance

*   **Use Job Compute for Automation:** The single most important cost optimization for production pipelines.
*   **Enable Photon:** A C++ based vectorized engine that dramatically speeds up SQL and DataFrame operations.
*   **Autoscaling and Auto-Termination:** Enable autoscaling on all clusters and set an auto-termination policy on interactive clusters.
*   **Use Spot Instances:** For fault-tolerant workloads, use spot instances for worker nodes.
*   **Right-Sizing:** Choose instance types that match your workload.
*   **Use Cluster Policies:** Enforce tagging, restrict DBR versions and instance types, and mandate auto-termination.
*   **Avoid Init Scripts:** Use cluster policies instead to avoid technical debt.
*   **Use Unity Catalog:** For centralized data governance and security.
